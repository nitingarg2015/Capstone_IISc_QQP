{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzN87nsmJ6x-",
        "outputId": "313d9f84-1785-425b-c8d0-4df0b464022b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 77840, done.\u001b[K\n",
            "remote: Counting objects: 100% (219/219), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 77840 (delta 129), reused 164 (delta 100), pack-reused 77621\u001b[K\n",
            "Receiving objects: 100% (77840/77840), 593.42 MiB | 48.71 MiB/s, done.\n",
            "Resolving deltas: 100% (55305/55305), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CEP2GROzIP12"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/models:/content/models/research/:/content/models/research/slim/\"\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/models:/content/models:/content/models/research/:/content/models/research/slim/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXBztJWhJAyh",
        "outputId": "60576904-0325-4e6b-9dc4-6b9266cd55da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf-models-official\n",
            "  Downloading tf_models_official-2.10.0-py2.py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.12.11)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[K     |████████████████████████████████| 238 kB 78.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (2.0.5)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.7.3)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (5.4.8)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.5.12)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (3.2.2)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.1.3)\n",
            "Collecting tf-slim>=1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 83.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow-text~=2.10.0\n",
            "  Downloading tensorflow_text-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 74.1 MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.6.0)\n",
            "Collecting immutabledict\n",
            "  Downloading immutabledict-2.2.1-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.3.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (7.1.2)\n",
            "Collecting tensorflow~=2.10.0\n",
            "  Downloading tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 578.0 MB 12 kB/s \n",
            "\u001b[?25hCollecting sacrebleu==2.2.0\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 86.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.29.32)\n",
            "Collecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 77.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.21.6)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 81.0 MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless==4.5.2.52\n",
            "  Downloading opencv_python_headless-4.5.2.52-cp37-cp37m-manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.2 MB 68.5 MB/s \n",
            "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 72.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.5.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu==2.2.0->tf-models-official) (2022.6.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu==2.2.0->tf-models-official) (0.8.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu==2.2.0->tf-models-official) (4.9.1)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.35.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.31.6)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2022.4)\n",
            "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.17.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (1.56.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.9)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (6.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (2022.9.24)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.64.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.10)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (1.6.3)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting keras<2.11,>=2.10.0\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 71.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (2.0.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (4.1.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (1.2.0)\n",
            "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 84.4 MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.11,>=2.10\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 71.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (0.27.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (22.9.24)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (1.49.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official) (14.0.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.10.0->tf-models-official) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.10.0->tf-models-official) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official) (1.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official) (3.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official) (3.2.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (1.4.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.2.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (5.9.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.3.5.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (1.10.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.8.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\n",
            "Building wheels for collected packages: py-cpuinfo, seqeval\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=615a9d905b4da51274d500dc2f36dd99232e6adfe3eb82eb27fd68f51cdafdd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=4902430ffacee391944d176226641c5f41a7f59eee1d2c2652c83d319dafd66a\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built py-cpuinfo seqeval\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, tensorflow, portalocker, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, py-cpuinfo, opencv-python-headless, immutabledict, tf-models-official\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220929150707\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220929150707:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220929150707\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.6.0.66\n",
            "    Uninstalling opencv-python-headless-4.6.0.66:\n",
            "      Successfully uninstalled opencv-python-headless-4.6.0.66\n",
            "Successfully installed colorama-0.4.5 gast-0.4.0 immutabledict-2.2.1 keras-2.10.0 opencv-python-headless-4.5.2.52 portalocker-2.5.1 py-cpuinfo-8.0.0 pyyaml-5.4.1 sacrebleu-2.2.0 sentencepiece-0.1.97 seqeval-1.2.2 tensorboard-2.10.1 tensorflow-2.10.0 tensorflow-addons-0.18.0 tensorflow-estimator-2.10.0 tensorflow-model-optimization-0.7.3 tensorflow-text-2.10.0 tf-models-official-2.10.0 tf-slim-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ytCr45C2MqM",
        "outputId": "e6ca9627-bc21-4c41-b77f-82a3e3b8fd4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 141 kB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.64.1)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30535 sha256=0ee7f70839068c7c00ec735d6fb887423ade435316f6752e2d4beaab462aad17\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19472 sha256=b6bbd0ddd3f5ec04a434689b30365217726d53921c88662cc41c92137026c56e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=13bc1459aad866edbbb20b5258a564fb4b076a0977b009fe99cd2d87df3fac8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 install bert-for-tf2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "96gOWtampvY_"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import math\n",
        "import collections\n",
        "import csv\n",
        "import os\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HHBD6r96m27"
      },
      "source": [
        "Import Tensorflow BERT and NLP packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LK4rmOBl9hqu"
      },
      "outputs": [],
      "source": [
        "from official.common import distribute_utils\n",
        "from official.nlp import optimization\n",
        "from official.nlp.modeling import networks\n",
        "from official.utils.misc import keras_utils\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_BbtO0U6oTO"
      },
      "source": [
        "Authentication for accessing Google Cloud storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tmEYYeiHuZa1"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sR8ssddMuy2O"
      },
      "outputs": [],
      "source": [
        "CLOUD_PROJECT = 'strategic-cacao-364520'\n",
        "BUCKET = 'gs://cds-sbert'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQi_sXx8w-O-",
        "outputId": "ce73e7a8-a7b7-4350-b760-9f5b0b5738f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "!gcloud config set project $CLOUD_PROJECT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kltICCGEaL_e"
      },
      "outputs": [],
      "source": [
        "bert_train_file = BUCKET +  \"/input-data/bctrain.txt\"\n",
        "bert_test_file = BUCKET +  \"/input-data/bcdev.txt\"\n",
        "model_dir = BUCKET + '/bert_model9'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkE5MQ777MiX"
      },
      "source": [
        "Initializing SBERT Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBb53U7PW_OQ",
        "outputId": "0d712f9c-ed07-430d-a74d-1210167cf716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU address is grpc://10.112.193.2:8470\n"
          ]
        }
      ],
      "source": [
        "train_data_path = bert_train_file\n",
        "eval_data_path = None\n",
        "init_checkpoint = None\n",
        "\n",
        "max_seq_length = 128\n",
        "\n",
        "train_batch_size = 32\n",
        "train_data_size = 363846\n",
        "eval_data_size = 10000\n",
        "num_train_epochs = 3\n",
        "eval_batch_size = 16\n",
        "\n",
        "pooling_mech = 'mean'                        # ['cls', 'mean', 'max']\n",
        "compute_similarity = 'dense'                 # ['cosine_similarity', 'dense']\n",
        "cmode = 'train_and_eval'                     # ['train_and_eval', 'predict']\n",
        "learning_rate = 3e-5\n",
        "vocab_size = 30522\n",
        "\n",
        "distribution_strategy = 'tpu'\n",
        "tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', tpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XicwSOo_7xI4"
      },
      "source": [
        "Functions to convert Questions text to Ids (tokens, segment and masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "whuxgyjSQAZq"
      },
      "outputs": [],
      "source": [
        "def sentence_to_ids(sentence, tokenizer, max_seq_length):\n",
        "    # Consider [CLS] and [SEP]\n",
        "    tokenized = tokenizer.tokenize(sentence)\n",
        "    tokens = [\"[CLS]\"]\n",
        "    tokens.extend(tokenized[0:(max_seq_length - 2)])\n",
        "    tokens.append(\"[SEP]\")\n",
        "\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    mask = [1] * len(ids)\n",
        "    seg_id = 0\n",
        "    segment_ids = [seg_id] * max_seq_length\n",
        "  \n",
        "    while len(ids) < max_seq_length:\n",
        "        ids.append(0)\n",
        "        mask.append(0)\n",
        "\n",
        "    assert len(ids) == max_seq_length\n",
        "    assert len(mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    def create_feature(values):\n",
        "        f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
        "        return f\n",
        "\n",
        "    return create_feature(ids), create_feature(mask), create_feature(segment_ids)\n",
        "\n",
        "def convert_single_example(features, tokenizer, max_seq_length):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "    assert len(features) == 6\n",
        "    idx = int(features[0])\n",
        "    sentence0 = features[3]\n",
        "    sentence1 = features[4]\n",
        "    label = float(features[5])\n",
        "\n",
        "    features = collections.OrderedDict()\n",
        "    features[\"id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[idx]))\n",
        "    (features[\"left_input_ids\"], features[\"left_input_mask\"],\n",
        "        features[\"left_segment_ids\"]) = sentence_to_ids(sentence0, tokenizer, max_seq_length)\n",
        "    (features[\"right_input_ids\"], features[\"right_input_mask\"],\n",
        "        features[\"right_segment_ids\"]) = sentence_to_ids(sentence1, tokenizer, max_seq_length)\n",
        "    features[\"label\"] = tf.train.Feature(float_list=tf.train.FloatList(value=[label]))\n",
        "\n",
        "    return tf.train.Example(features=tf.train.Features(feature=features)), label == 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m8khSgc9B5V"
      },
      "source": [
        "Function to prepare inputs to the SBERT Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VWN5i_O2aMYC"
      },
      "outputs": [],
      "source": [
        "def create_dataset(bert_input_file, seq_length, batch_size, is_training=True, input_pipeline_context=None):\n",
        "    \"\"\"Creates a TF dataset.\"\"\"\n",
        "    dataset = tf.data.TFRecordDataset(bert_input_file)\n",
        "\n",
        "    features = {\n",
        "      'left_input_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
        "      'left_input_mask': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
        "      'left_segment_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
        "      'right_input_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
        "      'right_input_mask': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
        "      'right_segment_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
        "      'label': tf.io.FixedLenFeature([], tf.float32),\n",
        "    }\n",
        "\n",
        "    dataset = dataset.map( lambda record: tf.io.parse_single_example(record, features), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    if input_pipeline_context and input_pipeline_context.num_input_pipelines > 1:\n",
        "        dataset = dataset.shard(input_pipeline_context.num_input_pipelines, input_pipeline_context.input_pipeline_id)\n",
        "\n",
        "    def _select_data_from_record(record):\n",
        "        x = {\n",
        "            'left_input_ids': record['left_input_ids'],\n",
        "            'left_input_mask': record['left_input_mask'],\n",
        "            'left_input_type_ids': record['left_segment_ids'],\n",
        "            'right_input_ids': record['right_input_ids'],\n",
        "            'right_input_mask': record['right_input_mask'],\n",
        "            'right_input_type_ids': record['right_segment_ids']\n",
        "        }\n",
        "        y = record['label']\n",
        "        return (x, y)\n",
        "\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(100)\n",
        "        dataset = dataset.repeat()\n",
        "\n",
        "    dataset = dataset.map(_select_data_from_record, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=is_training)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDoHwUYs88rm"
      },
      "source": [
        "SBERT Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AQfe3tR_RDJg"
      },
      "outputs": [],
      "source": [
        "class SBERT_Classifier(tf.keras.Model):\n",
        "    \"\"\"Sequence similarity classifier.\n",
        "        Args:\n",
        "        encoder: a tf.keras.Model defines an encoder network.\n",
        "        max_seq_length: maximum sequence length.\n",
        "        poolining: poolining mechanism. One of [cls, mean, max].\n",
        "        compute_similarity: how to compute the similarity probability outputs. One of [cosine_similarity, dense].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, max_seq_length, dropout_rate=0.1, pooling='mean', compute_similarity='dense', **kwargs):\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.pooling = pooling\n",
        "        self.compute_similarity = compute_similarity\n",
        "        \n",
        "        # Prepare inputs\n",
        "        left_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='left_word_ids')\n",
        "        left_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='left_mask')\n",
        "        left_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='left_type_ids')\n",
        "        left_inputs = [left_word_ids, left_mask, left_type_ids]\n",
        "\n",
        "        right_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='right_word_ids')\n",
        "        right_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='right_mask')\n",
        "        right_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='right_type_ids')\n",
        "  \n",
        "        # Normally the question word is the second token.\n",
        "        left_q_word = tf.slice(left_word_ids, [0, 1], [-1, 1])\n",
        "        right_q_word = tf.slice(right_word_ids, [0, 1], [-1, 1])\n",
        "        question_word_diff = tf.cast(tf.not_equal(left_q_word, right_q_word), tf.float32)\n",
        "\n",
        "        right_inputs = [right_word_ids, right_mask, right_type_ids]\n",
        "        inputs = {\n",
        "            'left_input_ids': left_word_ids,\n",
        "            'left_input_mask': left_mask,\n",
        "            'left_input_type_ids': left_type_ids,\n",
        "            'right_input_ids': right_word_ids,\n",
        "            'right_input_mask': right_mask,\n",
        "            'right_input_type_ids': right_type_ids\n",
        "        }\n",
        "\n",
        "        expanded_left_mask = tf.expand_dims(tf.cast(left_mask, dtype=tf.float32), axis=2)\n",
        "        expanded_right_mask = tf.expand_dims(tf.cast(right_mask, dtype=tf.float32), axis=2)\n",
        "      \n",
        "        # Get sequence embeddings\n",
        "        left_sequence_output, left_cls_output = encoder(left_inputs)\n",
        "        right_sequence_output, right_cls_output = encoder(right_inputs)\n",
        "        left_sequence_output = left_sequence_output * expanded_left_mask\n",
        "        right_sequence_output = right_sequence_output * expanded_right_mask\n",
        "\n",
        "        # Pooling\n",
        "        if pooling == 'cls':\n",
        "            # Outputs on the first token 'CLs'\n",
        "            left_outputs = left_cls_output\n",
        "            right_outputs = right_cls_output\n",
        "        elif pooling == 'mean':\n",
        "            left_outputs = tf.reduce_sum(left_sequence_output, axis=1) / tf.reduce_sum(expanded_left_mask, axis=1)\n",
        "            right_outputs = tf.reduce_sum(right_sequence_output, axis=1) / tf.reduce_sum(expanded_right_mask, axis=1)\n",
        "        elif pooling == 'max':\n",
        "            left_outputs = tf.reduce_max(left_sequence_output, axis=1)\n",
        "            right_outputs = tf.reduce_max(right_sequence_output, axis=1)\n",
        "        else:\n",
        "            raise ValueError('Pooling %s is not supported: %s' % pooling)\n",
        "\n",
        "        # Compute similarity\n",
        "        if compute_similarity == 'cosine_similarity':\n",
        "            cos_similarity = tf.reduce_sum(tf.nn.l2_normalize(left_outputs, axis=1) * tf.nn.l2_normalize(right_outputs, axis=1), axis=1)\n",
        "            prob = (cos_similarity + 1) / 2\n",
        "        elif compute_similarity == 'dense':\n",
        "            concat_outputs = tf.concat([left_outputs, right_outputs, tf.abs(left_outputs - right_outputs)], axis=1)\n",
        "            outputs = tf.keras.layers.Dense(128, activation='relu')(concat_outputs)\n",
        "            # skip connection on question words\n",
        "            outputs = tf.concat([outputs, question_word_diff], axis=1)\n",
        "            prob = tf.keras.layers.Dense(1, activation='sigmoid')(outputs)\n",
        "        else:\n",
        "            raise ValueError('compute_similarity %s is not supported: %s' %compute_similarity)\n",
        "   \n",
        "        super(SBERT_Classifier, self).__init__(inputs=inputs, outputs=prob, **kwargs)\n",
        "        self._encoder = encoder\n",
        "\n",
        "        config_dict = {\n",
        "            'encoder': self._encoder,\n",
        "            'max_seq_length': self.max_seq_length,\n",
        "            'pooling': self.pooling,\n",
        "            'compute_similarity': self.compute_similarity\n",
        "        }\n",
        "        config_cls = collections.namedtuple('Config', config_dict.keys())\n",
        "        self._config = config_cls(**config_dict)\n",
        "\n",
        "    @property\n",
        "    def checkpoint_items(self):\n",
        "        items = dict(encoder=self._encoder)\n",
        "        return items\n",
        "\n",
        "    def get_config(self):\n",
        "         return dict(self._config._asdict())\n",
        "        \n",
        "    @classmethod\n",
        "    def from_config(cls, config, custom_objects=None):\n",
        "        return cls(**config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8D6zEky9VK3"
      },
      "source": [
        "Functions to run SBERT Classifier, Trian and Evaluate, Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y-l1tXc2XJRd"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "def get_loss_fn():\n",
        "    \"\"\"Gets the binary classification loss function.\"\"\"\n",
        "    def classification_loss_fn(labels, logits):\n",
        "        binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(\n",
        "            label_smoothing=0.1, reduction=tf.keras.losses.Reduction.SUM)\n",
        "        return binary_cross_entropy(labels, logits)\n",
        "    return classification_loss_fn\n",
        "\n",
        "def get_dataset_fn(input_file, max_seq_length, global_batch_size, is_training):\n",
        "    \"\"\"Gets a closure to create a dataset.\"\"\"\n",
        "    def _dataset_fn(ctx=None):\n",
        "        batch_size = ctx.get_per_replica_batch_size(global_batch_size) if ctx else global_batch_size\n",
        "        ds = create_dataset(input_file, max_seq_length, batch_size, is_training=is_training, input_pipeline_context=ctx)\n",
        "        return ds\n",
        "    return _dataset_fn\n",
        "\n",
        "def run_bertclassifier(strategy, model_dir, epochs, steps_per_epoch, eval_steps,\n",
        "                            warmup_steps, init_checkpoint, train_input_fn, eval_input_fn):\n",
        "    \"\"\"Train the SBERT classifier.\"\"\"\n",
        "  \n",
        "    pooling = pooling_mech\n",
        "\n",
        "    loss_fn = (get_loss_fn())\n",
        "   \n",
        "    # Start training using Keras compile/fit API.\n",
        "    with strategy.scope():\n",
        "        training_dataset = train_input_fn()\n",
        "        evaluation_dataset = eval_input_fn() if eval_input_fn else None\n",
        "        enc = networks.BertEncoder(vocab_size=vocab_size, num_layers=12, type_vocab_size=2)\n",
        "        classifier = SBERT_Classifier(enc, max_seq_length, pooling=pooling, compute_similarity=compute_similarity)\n",
        "        classifier.optimizer = optimization.create_optimizer(learning_rate, steps_per_epoch * epochs, warmup_steps)\n",
        "        optimizer = classifier.optimizer\n",
        "\n",
        "        if init_checkpoint:\n",
        "            # load pretrained weights\n",
        "            classifier.load_weights(init_checkpoint).assert_existing_objects_matched()\n",
        "        \n",
        "        loss_weights = None\n",
        "        metrics_list = [tf.metrics.BinaryAccuracy(), tf.metrics.Precision(), tf.metrics.Recall()]\n",
        "\n",
        "        classifier.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=loss_fn,\n",
        "            loss_weights=loss_weights,\n",
        "            metrics=metrics_list, \n",
        "            steps_per_execution=100)\n",
        "\n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=model_dir, save_weights_only=True,  verbose=1)\n",
        "        \n",
        "        callbacks = [cp_callback]\n",
        "\n",
        "        history = classifier.fit(\n",
        "            x=training_dataset,\n",
        "            validation_data=evaluation_dataset,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=epochs,\n",
        "            validation_steps=eval_steps,\n",
        "            callbacks=callbacks)\n",
        "        stats = {'total_training_steps': steps_per_epoch * epochs}\n",
        "        if 'loss' in history.history:\n",
        "            stats['train_loss'] = history.history['loss'][-1]\n",
        "        if 'val_accuracy' in history.history:\n",
        "            stats['eval_metrics'] = history.history['val_accuracy'][-1]\n",
        "        return classifier, stats\n",
        "\n",
        "def train_and_eval(strategy, train_input_fn=None, eval_input_fn=None, init_checkpoint=None):\n",
        "    \"\"\"Run training and evaluation.\"\"\"\n",
        "    epochs = num_train_epochs\n",
        "    steps_per_epoch = int(train_data_size // train_batch_size)\n",
        "    warmup_steps = int(epochs * train_data_size * 0.1 // train_batch_size)\n",
        "    eval_steps = int(math.ceil(eval_data_size // eval_batch_size))\n",
        "\n",
        "    trained_model, _ = run_bertclassifier(strategy, model_dir, epochs, steps_per_epoch, eval_steps, warmup_steps,\n",
        "                                      init_checkpoint, train_input_fn, eval_input_fn)\n",
        "\n",
        "    return trained_model\n",
        "\n",
        "def get_predictions_and_labels(strategy, trained_model, eval_input_fn):\n",
        "    \"\"\"Obtains predictions of trained model on test data.\"\"\"\n",
        "\n",
        "    @tf.function\n",
        "    def predict_step(iterator):\n",
        "        \"\"\"Computes predictions on distributed devices.\"\"\"\n",
        "\n",
        "        def _predict_step_fn(inputs):\n",
        "            \"\"\"Replicated predictions.\"\"\"\n",
        "            inputs, labels = inputs\n",
        "            probabilities = trained_model(inputs, training=False)\n",
        "            return probabilities, labels\n",
        "\n",
        "        outputs, labels = strategy.run(_predict_step_fn, args=(next(iterator),))\n",
        "        outputs = tf.nest.map_structure(strategy.experimental_local_results, outputs)\n",
        "        labels = tf.nest.map_structure(strategy.experimental_local_results, labels)\n",
        "        return outputs, labels\n",
        "\n",
        "    test_iter = iter(strategy.distribute_datasets_from_function(eval_input_fn))\n",
        "    all_predictions, all_labels = list(), list()\n",
        "    try:\n",
        "        with tf.experimental.async_scope():\n",
        "            while True:\n",
        "                probs, labels = predict_step(test_iter)\n",
        "                for prob, label in zip(probs, labels):\n",
        "                    all_predictions.extend(prob.numpy())\n",
        "                    all_labels.extend(label.numpy())\n",
        "    except (StopIteration, tf.errors.OutOfRangeError):\n",
        "        tf.experimental.async_clear_error()\n",
        "\n",
        "    return all_predictions, all_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp8jgSiD4wI0"
      },
      "source": [
        "Request cloud TPU environment to run same Tensorflow version as current colab notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfwWvRXSQIGL",
        "outputId": "f32e20b5-93f8-4676-ad1a-78b46439aa58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/2.10.0'\n",
        "resp = requests.post(url)\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB6WGqSF43Rr"
      },
      "source": [
        "Set TPU distribution strategy to train Sbert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPCUAs7Tq8FP",
        "outputId": "9f5791e0-0454-4b52-b0a7-889f9261530c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.112.193.2:8470']\n",
            "REPLICAS:  8\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ago-NJEFfzxD",
        "outputId": "616cf55a-237f-49e4-f081-fba7fdb598bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['pooler_transform/kernel:0', 'pooler_transform/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['pooler_transform/kernel:0', 'pooler_transform/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['pooler_transform/kernel:0', 'pooler_transform/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['pooler_transform/kernel:0', 'pooler_transform/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11370/11370 [==============================] - ETA: 0s - loss: 0.9670 - binary_accuracy: 0.9777 - precision_1: 0.9603 - recall_1: 0.9801\n",
            "Epoch 1: saving model to gs://cds-sbert/bert_model9\n",
            "11370/11370 [==============================] - 797s 70ms/step - loss: 0.9670 - binary_accuracy: 0.9777 - precision_1: 0.9603 - recall_1: 0.9801\n",
            "Epoch 2/3\n",
            "11370/11370 [==============================] - ETA: 0s - loss: 0.9680 - binary_accuracy: 0.9776 - precision_1: 0.9602 - recall_1: 0.9801\n",
            "Epoch 2: saving model to gs://cds-sbert/bert_model9\n",
            "11370/11370 [==============================] - 728s 64ms/step - loss: 0.9680 - binary_accuracy: 0.9776 - precision_1: 0.9602 - recall_1: 0.9801\n",
            "Epoch 3/3\n",
            "11370/11370 [==============================] - ETA: 0s - loss: 0.9688 - binary_accuracy: 0.9774 - precision_1: 0.9601 - recall_1: 0.9795\n",
            "Epoch 3: saving model to gs://cds-sbert/bert_model9\n",
            "11370/11370 [==============================] - 729s 64ms/step - loss: 0.9688 - binary_accuracy: 0.9774 - precision_1: 0.9601 - recall_1: 0.9795\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Train the classification Model.\"\"\"\n",
        "eval_input_fn = None \n",
        "#init_checkpoint = None\n",
        "init_checkpoint = model_dir\n",
        "train_input_fn = get_dataset_fn(train_data_path, max_seq_length, train_batch_size, is_training=True)\n",
        "bertModel = train_and_eval(strategy, train_input_fn, eval_input_fn, init_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict on Test data"
      ],
      "metadata": {
        "id": "-pL1ZR-8HHBK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zrIzMdM_hXvR"
      },
      "outputs": [],
      "source": [
        "test_input_fn = get_dataset_fn(bert_test_file, max_seq_length, train_batch_size, is_training=False)\n",
        "preds, labels = get_predictions_and_labels(strategy, bertModel, test_input_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# probability threshold of 70% for positive duplicate\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
        "import numpy as np\n",
        "ypred = np.array(preds).reshape(-1)\n",
        "rundoff = lambda x: 1 if x > 0.7 else 0\n",
        "vfunc = np.vectorize(rundoff)\n",
        "ypreds = vfunc(ypred)\n",
        "#prediction1 = np.round(abs(ypred))\n",
        "print ('Accuracy:  ', accuracy_score(labels, ypreds))\n",
        "print ('F1 score:  ', f1_score(labels, ypreds))\n",
        "print ('Recall:    ', recall_score(labels, ypreds))\n",
        "print ('Precision: ', precision_score(labels, ypreds))\n",
        "print ('\\nclasification report:\\n\\n', classification_report(labels,ypreds))\n",
        "print ('\\n confusion matrix:\\n\\n',confusion_matrix(labels, ypreds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B46DqAzb08q9",
        "outputId": "90ab6dd2-af6e-4a75-aa7c-bbaf8a22a0d3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:   0.8557754142963147\n",
            "F1 score:   0.8100342075256557\n",
            "Recall:     0.8352032247228753\n",
            "Precision:  0.786337760910816\n",
            "\n",
            "clasification report:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.87      0.88     25545\n",
            "         1.0       0.79      0.84      0.81     14885\n",
            "\n",
            "    accuracy                           0.86     40430\n",
            "   macro avg       0.84      0.85      0.85     40430\n",
            "weighted avg       0.86      0.86      0.86     40430\n",
            "\n",
            "\n",
            " confusion matrix:\n",
            "\n",
            " [[22167  3378]\n",
            " [ 2453 12432]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWOYpXjcjXQ_",
        "outputId": "50f8f08d-0687-489e-aea5-10a4a172553b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:   0.8569131832797428\n",
            "F1 score:   0.8103713901727472\n",
            "Recall:     0.8304333221363789\n",
            "Precision:  0.7912559211368583\n",
            "\n",
            "clasification report:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.87      0.89     25545\n",
            "         1.0       0.79      0.83      0.81     14885\n",
            "\n",
            "    accuracy                           0.86     40430\n",
            "   macro avg       0.84      0.85      0.85     40430\n",
            "weighted avg       0.86      0.86      0.86     40430\n",
            "\n",
            "\n",
            " confusion matrix:\n",
            "\n",
            " [[22284  3261]\n",
            " [ 2524 12361]]\n"
          ]
        }
      ],
      "source": [
        "# probability threshold of 75% for positive duplicate\n",
        "\n",
        "ypred = np.array(preds).reshape(-1)\n",
        "rundoff = lambda x: 1 if x > 0.75 else 0\n",
        "vfunc = np.vectorize(rundoff)\n",
        "ypreds = vfunc(ypred)\n",
        "#prediction1 = np.round(abs(ypred))\n",
        "print ('Accuracy:  ', accuracy_score(labels, ypreds))\n",
        "print ('F1 score:  ', f1_score(labels, ypreds))\n",
        "print ('Recall:    ', recall_score(labels, ypreds))\n",
        "print ('Precision: ', precision_score(labels, ypreds))\n",
        "print ('\\nclasification report:\\n\\n', classification_report(labels,ypreds))\n",
        "print ('\\n confusion matrix:\\n\\n',confusion_matrix(labels, ypreds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# probability threshold of 80% for positive duplicate\n",
        "\n",
        "ypred = np.array(preds).reshape(-1)\n",
        "rundoff = lambda x: 1 if x > 0.8 else 0\n",
        "vfunc = np.vectorize(rundoff)\n",
        "ypreds = vfunc(ypred)\n",
        "#prediction1 = np.round(abs(ypred))\n",
        "print ('Accuracy:  ', accuracy_score(labels, ypreds))\n",
        "print ('F1 score:  ', f1_score(labels, ypreds))\n",
        "print ('Recall:    ', recall_score(labels, ypreds))\n",
        "print ('Precision: ', precision_score(labels, ypreds))\n",
        "print ('\\nclasification report:\\n\\n', classification_report(labels,ypreds))\n",
        "print ('\\n confusion matrix:\\n\\n',confusion_matrix(labels, ypreds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO158XlO4PGi",
        "outputId": "4b281e52-75e5-4207-f658-fc7f973cfe62"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:   0.857803611179817\n",
            "F1 score:   0.8102327116685921\n",
            "Recall:     0.8245213301981861\n",
            "Precision:  0.7964308890330954\n",
            "\n",
            "clasification report:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.88      0.89     25545\n",
            "         1.0       0.80      0.82      0.81     14885\n",
            "\n",
            "    accuracy                           0.86     40430\n",
            "   macro avg       0.85      0.85      0.85     40430\n",
            "weighted avg       0.86      0.86      0.86     40430\n",
            "\n",
            "\n",
            " confusion matrix:\n",
            "\n",
            " [[22408  3137]\n",
            " [ 2612 12273]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "list_of_tuples = list(zip(preds, ypreds,labels))\n",
        "tst = pd.DataFrame(list_of_tuples, columns=['prob','pred', 'label'])\n",
        "\n",
        "tst.to_csv('sbert_predicted_75_output.csv')"
      ],
      "metadata": {
        "id": "WKmZwiAnxX-3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcePS5gMjAu6"
      },
      "source": [
        "Misc - backup for creating Word tags, Segments and Masks for train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNsRHPXThXFK"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    enc = networks.BertEncoder(vocab_size=vocab_size, num_layers=12, type_vocab_size=2)\n",
        "    base_classifier = SBERT_Classifier(enc, max_seq_length, pooling=pooling_mech, compute_similarity=compute_similarity,\n",
        "                                    question_word_penalty_weight=question_word_penalty_weight, pred_threshold=pred_threshold)\n",
        "classifier = base_classifier\n",
        "checkpoint = tf.train.Checkpoint(model=classifier)\n",
        "latest_checkpoint_file = (init_checkpoint or tf.train.latest_checkpoint(model_dir))\n",
        "assert latest_checkpoint_file\n",
        "logging.info('Checkpoint file %s found and restoring from '\n",
        "            'checkpoint', latest_checkpoint_file)\n",
        "checkpoint.restore(latest_checkpoint_file).assert_existing_objects_matched()\n",
        "preds, labels = get_predictions_and_labels(strategy, classifier, eval_input_fn)\n",
        "#write_prediction_outputs(preds, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9ZAoBR9hCBO"
      },
      "source": [
        "Backup to create Train and Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgVaUJv5OhO3",
        "outputId": "b5c34f1b-f064-4cc5-bcdb-d40928a7608e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29EuwTt56RHA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train1_data = pd.read_csv(input_file, delimiter=\",\", encoding='utf-8')\n",
        "train_data = train1_data.sample(frac=1)\n",
        "train=train1_data.sample(frac=0.8,random_state=42) #random state is a seed value\n",
        "test=train1_data.drop(train.index)\n",
        "\n",
        "train.to_csv(r'/content/drive/MyDrive/Copy-CDS-B3/Capstone Project/Data/cleantext/bert_train_data.csv', index=None, sep=',', mode='a')\n",
        "test.to_csv(r'/content/drive/MyDrive/Copy-CDS-B3/Capstone Project/Data/cleantext/bert_test_data.csv', index=None, sep=',', mode='a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf0iqsW2OO5e"
      },
      "outputs": [],
      "source": [
        "input_file = \"/content/drive/MyDrive/Copy-CDS-B3/Capstone Project/Data/cleantext/bert/dev.tsv\"\n",
        "output_file = \"/content/drive/MyDrive/Copy-CDS-B3/Capstone Project/Data/cleantext/bert/bdev.txt\"\n",
        "vocab_file = \"/content/drive/MyDrive/Copy-CDS-B3/Capstone Project/Data/cleantext/bert/vocab.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HjljfArQC1x"
      },
      "outputs": [],
      "source": [
        "writer = tf.io.TFRecordWriter(output_file)\n",
        "\n",
        "from bert import bert_tokenization\n",
        "\n",
        "example_idx = 0\n",
        "logging.info(\"Start.\")\n",
        "pos_num = 0\n",
        "  \n",
        "# WordPiece tokenization\n",
        "tokenizer = bert_tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n",
        "\n",
        "with tf.io.gfile.GFile(input_file, \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "    next(reader)\n",
        "    examples = []\n",
        "    for features in reader:\n",
        "        tf_example, pos = convert_single_example(features, tokenizer, max_seq_length)\n",
        "        if pos:\n",
        "            pos_num = pos_num + 1\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "        example_idx = example_idx + 1\n",
        "        if example_idx % 10000 == 0:\n",
        "            logging.info(\"Finished writing %d examples\", example_idx)\n",
        "        logging.info(\"Finished writing %d examples\", example_idx)\n",
        "        logging.info(\"Positive %d examples\", pos_num)\n",
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}